"""
RMS24 Hint Generation - Forge-optimized CUDA kernel.

Generated by Forge (rightnowai.co/forge) from forge_minimal.py.
Claims 56.5x speedup over PyTorch reference.

Key optimizations:
- 256 threads per block (8 warps) instead of 32
- Strided iteration for better occupancy with large subsets
- Two-level reduction: warp shuffle + shared memory across warps
- Coalesced memory access pattern
"""

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline


class PytorchModel(nn.Module):
    """Reference PyTorch implementation (for correctness comparison)."""
    
    def __init__(self):
        super().__init__()
    
    def forward(
        self,
        entries: torch.Tensor,       # [num_entries, 5] int64
        padded_indices: torch.Tensor, # [num_hints, max_subset_size] int64
        valid_mask: torch.Tensor,     # [num_hints, max_subset_size] bool
    ) -> torch.Tensor:
        gathered = entries[padded_indices]
        gathered = gathered * valid_mask.unsqueeze(-1).to(gathered.dtype)
        
        parity = gathered[:, 0, :].clone()
        for i in range(1, gathered.shape[1]):
            parity = torch.bitwise_xor(parity, gathered[:, i, :])
        
        return parity


cuda_source = '''
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <stdint.h>

// Each block handles one hint
// Threads within a block cooperatively XOR entries
__global__ void hint_gen_kernel(
    const int64_t* __restrict__ entries,        // [num_entries, 5]
    const int64_t* __restrict__ padded_indices, // [num_hints, max_subset_size]
    const bool* __restrict__ valid_mask,        // [num_hints, max_subset_size]
    int64_t* __restrict__ parities,             // [num_hints, 5]
    int num_entries,
    int num_hints,
    int max_subset_size
) {
    int hint_idx = blockIdx.x;
    if (hint_idx >= num_hints) return;
    
    int tid = threadIdx.x;
    int num_threads = blockDim.x;
    
    // Shared memory for partial XOR results - each warp writes 5 values
    __shared__ int64_t shared_parity[5 * 32];  // 32 warps max
    
    int warp_id = tid / 32;
    int lane_id = tid % 32;
    int num_warps = (num_threads + 31) / 32;
    
    // Local accumulator for this thread
    int64_t local_parity[5] = {0, 0, 0, 0, 0};
    
    // Pointers to this hint's data
    const int64_t* hint_indices = padded_indices + hint_idx * max_subset_size;
    const bool* hint_mask = valid_mask + hint_idx * max_subset_size;
    
    // Each thread processes a strided subset of indices
    for (int i = tid; i < max_subset_size; i += num_threads) {
        if (hint_mask[i]) {
            int64_t entry_idx = hint_indices[i];
            const int64_t* entry = entries + entry_idx * 5;
            
            local_parity[0] ^= entry[0];
            local_parity[1] ^= entry[1];
            local_parity[2] ^= entry[2];
            local_parity[3] ^= entry[3];
            local_parity[4] ^= entry[4];
        }
    }
    
    // Warp-level reduction using shuffle
    #pragma unroll
    for (int offset = 16; offset > 0; offset /= 2) {
        local_parity[0] ^= __shfl_xor_sync(0xffffffff, local_parity[0], offset);
        local_parity[1] ^= __shfl_xor_sync(0xffffffff, local_parity[1], offset);
        local_parity[2] ^= __shfl_xor_sync(0xffffffff, local_parity[2], offset);
        local_parity[3] ^= __shfl_xor_sync(0xffffffff, local_parity[3], offset);
        local_parity[4] ^= __shfl_xor_sync(0xffffffff, local_parity[4], offset);
    }
    
    // Lane 0 of each warp writes to shared memory
    if (lane_id == 0) {
        shared_parity[warp_id * 5 + 0] = local_parity[0];
        shared_parity[warp_id * 5 + 1] = local_parity[1];
        shared_parity[warp_id * 5 + 2] = local_parity[2];
        shared_parity[warp_id * 5 + 3] = local_parity[3];
        shared_parity[warp_id * 5 + 4] = local_parity[4];
    }
    
    __syncthreads();
    
    // Final reduction by first warp - each of first 5 threads handles one parity element
    if (warp_id == 0 && lane_id < 5) {
        int64_t final_val = 0;
        for (int w = 0; w < num_warps; w++) {
            final_val ^= shared_parity[w * 5 + lane_id];
        }
        parities[hint_idx * 5 + lane_id] = final_val;
    }
}

torch::Tensor forward(
    torch::Tensor entries,
    torch::Tensor padded_indices,
    torch::Tensor valid_mask
) {
    TORCH_CHECK(entries.is_cuda(), "entries must be CUDA tensor");
    TORCH_CHECK(padded_indices.is_cuda(), "padded_indices must be CUDA tensor");
    TORCH_CHECK(valid_mask.is_cuda(), "valid_mask must be CUDA tensor");
    
    entries = entries.contiguous();
    padded_indices = padded_indices.contiguous();
    valid_mask = valid_mask.contiguous();
    
    int num_entries = entries.size(0);
    int num_hints = padded_indices.size(0);
    int max_subset_size = padded_indices.size(1);
    
    auto parities = torch::zeros({num_hints, 5}, entries.options());
    
    // 256 threads per block (8 warps)
    int threads = 256;
    int blocks = num_hints;
    
    hint_gen_kernel<<<blocks, threads>>>(
        entries.data_ptr<int64_t>(),
        padded_indices.data_ptr<int64_t>(),
        valid_mask.data_ptr<bool>(),
        parities.data_ptr<int64_t>(),
        num_entries,
        num_hints,
        max_subset_size
    );
    
    return parities;
}
'''

cpp_source = '''
torch::Tensor forward(
    torch::Tensor entries,
    torch::Tensor padded_indices,
    torch::Tensor valid_mask
);
'''

# Lazy-load CUDA module
_cuda_module = None

def get_cuda_module():
    global _cuda_module
    if _cuda_module is None:
        _cuda_module = load_inline(
            name='hint_gen_forge',
            cpp_sources=cpp_source,
            cuda_sources=cuda_source,
            functions=['forward'],
            verbose=False,
            extra_cuda_cflags=['-O3', '--use_fast_math'],
        )
    return _cuda_module


class CUDAModel(nn.Module):
    """Forge-optimized CUDA kernel."""
    
    def __init__(self):
        super().__init__()
        self._module = None
    
    def forward(
        self,
        entries: torch.Tensor,
        padded_indices: torch.Tensor,
        valid_mask: torch.Tensor,
    ) -> torch.Tensor:
        if self._module is None:
            self._module = get_cuda_module()
        return self._module.forward(entries, padded_indices, valid_mask)


# Alias for compatibility
HintGenKernel = CUDAModel


def get_example_inputs(device="cuda", num_entries=262144, num_hints=100, max_subset_size=512):
    """Generate example inputs for benchmarking."""
    entries = torch.randint(
        0, 2**60, (num_entries, 5),
        dtype=torch.int64, device=device
    )
    padded_indices = torch.randint(
        0, num_entries, (num_hints, max_subset_size),
        dtype=torch.int64, device=device
    )
    valid_mask = torch.rand(num_hints, max_subset_size, device=device) < 0.8
    
    return entries, padded_indices, valid_mask


if __name__ == "__main__":
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    if device == "cpu":
        print("CUDA not available, using PyTorch reference")
        model = PytorchModel()
    else:
        print("Using Forge-optimized CUDA kernel")
        model = CUDAModel()
    
    inputs = get_example_inputs(device)
    output = model(*inputs)
    print(f"Output shape: {output.shape}")
    print(f"Device: {device}")
